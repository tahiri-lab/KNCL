{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbvpAMztd8pV"
   },
   "source": [
    "# Comparison between tree completion (by k-NCL) and tree pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BioZ5LMcyxc9"
   },
   "source": [
    "BSD(k-NCL) vs. BSD(-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BioZ5LMcyxc9"
   },
   "source": [
    "Three scenarios are evaluated based on how each distance interprets the similarity between two trees, $T_1$ and $T_2$, relative to a reference tree $T^*$ (the constructed supertree).\n",
    "\n",
    "* **Scenario 1 — disagreement in ordering:**\n",
    "\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  \\text{BSD}(k\\text{-NCL})(T_1, T^*) &< \\text{BSD}(k\\text{-NCL})(T_2, T^*) \\\\\n",
    "  \\text{BSD}(-)(T_1, T^*) &> \\text{BSD}(-)(T_2, T^*)\n",
    "  \\end{aligned}\n",
    "  \\quad \\text{or} \\quad\n",
    "  \\begin{aligned}\n",
    "  \\text{BSD}(k\\text{-NCL})(T_2, T^*) &< \\text{BSD}(k\\text{-NCL})(T_1, T^*) \\\\\n",
    "  \\text{BSD}(-)(T_2, T^*) &> \\text{BSD}(-)(T_1, T^*)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "* **Scenario 2 — different $k$-NCL distances, same BSD(-):**\n",
    "\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  \\text{BSD}(k\\text{-NCL})(T_1, T^*) &\\neq \\text{BSD}(k\\text{-NCL})(T_2, T^*) \\\\\n",
    "  \\text{BSD}(-)(T_1, T^*) &= \\text{BSD}(-)(T_2, T^*)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "* **Scenario 3 — same $k$-NCL distance, different BSD(-):**\n",
    "\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  \\text{BSD}(k\\text{-NCL})(T_1, T^*) &= \\text{BSD}(k\\text{-NCL})(T_2, T^*) \\\\\n",
    "  \\text{BSD}(-)(T_1, T^*) &\\neq \\text{BSD}(-)(T_2, T^*)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "The results include a table, line graphs, and violin charts showing the proportion of conflicts for each overlap level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPlRt9CKp1vC"
   },
   "source": [
    "### k-NCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use the k-ncl script available on GitHub\n",
    "\n",
    "\"\"\"\n",
    "This script implements the k-NCL algorithm for completing phylogenetic trees\n",
    "that are defined on different but overlapping taxon sets. The implementation\n",
    "uses the ete3 library to work with phylogenetic trees.\n",
    "\n",
    "Requires:  ete3  (pip install ete3)\n",
    "\"\"\"\n",
    "\n",
    "#from kncl import kNCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFxpQ0plp7Py"
   },
   "source": [
    "### BSD(k-NCL) / BSD(+) and BSD(-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "def squared_distance_sum_fast_ete3(t1, t2, leaf_names):\n",
    "    leaf_list = list(leaf_names)\n",
    "    name_to_node_t1 = {leaf.name: leaf for leaf in t1.iter_leaves()}\n",
    "    name_to_node_t2 = {leaf.name: leaf for leaf in t2.iter_leaves()}\n",
    "\n",
    "    # Cache node-to-root distances for both trees\n",
    "    dist_to_root_t1 = {name: node.get_distance(t1) for name, node in name_to_node_t1.items()}\n",
    "    dist_to_root_t2 = {name: node.get_distance(t2) for name, node in name_to_node_t2.items()}\n",
    "\n",
    "    total = 0\n",
    "    for i in range(len(leaf_list)):\n",
    "        for j in range(i + 1, len(leaf_list)):\n",
    "            a, b = leaf_list[i], leaf_list[j]\n",
    "\n",
    "            # Tree 1: get distance via LCA\n",
    "            lca1 = t1.get_common_ancestor(a, b)\n",
    "            d1 = dist_to_root_t1[a] + dist_to_root_t1[b] - 2 * lca1.get_distance(t1)\n",
    "\n",
    "            # Tree 2: get distance via LCA\n",
    "            lca2 = t2.get_common_ancestor(a, b)\n",
    "            d2 = dist_to_root_t2[a] + dist_to_root_t2[b] - 2 * lca2.get_distance(t2)\n",
    "\n",
    "            total += (d1 - d2) ** 2\n",
    "\n",
    "    return total\n",
    "\n",
    "def BSD(T1, T2, k=None):\n",
    "    leaves1 = {leaf.name for leaf in T1.iter_leaves()}\n",
    "    leaves2 = {leaf.name for leaf in T2.iter_leaves()}\n",
    "    common_leaves = leaves1 & leaves2\n",
    "\n",
    "    if len(common_leaves) < 3:\n",
    "        return None, None\n",
    "\n",
    "    T1_completed, T2_completed = kNCL(T1, T2, k)\n",
    "\n",
    "    completed_leaves = {leaf.name for leaf in T1_completed.iter_leaves()}\n",
    "\n",
    "    bsd_plus = math.sqrt(squared_distance_sum_fast_ete3(T1_completed, T2_completed, completed_leaves))\n",
    "    bsd_minus = math.sqrt(squared_distance_sum_fast_ete3(T1_completed, T2_completed, common_leaves))\n",
    "\n",
    "    return bsd_plus, bsd_minus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree completion versus tree pruning comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import tempfile\n",
    "import signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import comb\n",
    "from ete3 import Tree\n",
    "\n",
    "# CONFIG\n",
    "CHECKPOINT_EVERY = 1\n",
    "USE_PARALLEL = True    # set False to disable joblib parallelism\n",
    "N_JOBS = max(1, (os.cpu_count() or 2) - 1)  # workers for BSD precompute\n",
    "REPORT_INTERVAL = 2000  # print progress every N qualifying pairs\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), \"_checkpoints\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def save_atomic(path, obj, retries=10, delay=0.1):\n",
    "    \"\"\"\n",
    "    Write a pickle with Windows-friendly retries.\n",
    "    - Writes to a temp file in the same directory, fsyncs it, then replaces.\n",
    "    - Retries on PermissionError (file lock).\n",
    "    \"\"\"\n",
    "    # Destination and directory\n",
    "    dest = os.path.abspath(path)\n",
    "    d = os.path.dirname(dest) or \".\"\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Create temp file in the same directory\n",
    "    fd, tmp = tempfile.mkstemp(dir=d, prefix=\".tmp_ckpt_\", suffix=\".pkl\")\n",
    "    try:\n",
    "        # Write + flush + fsync to the temp file\n",
    "        with os.fdopen(fd, \"wb\") as f:\n",
    "            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "\n",
    "        # Try to replace with retries\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                os.replace(tmp, dest)\n",
    "                return  # success\n",
    "            except PermissionError:\n",
    "                # brief backoff\n",
    "                time.sleep(delay * (2 ** attempt))\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(dest):\n",
    "                os.remove(dest)\n",
    "            os.replace(tmp, dest)\n",
    "            return\n",
    "        finally:\n",
    "            # If replace failed, clean up temp file\n",
    "            if os.path.exists(tmp):\n",
    "                try:\n",
    "                    os.remove(tmp)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    except Exception:\n",
    "        # On any unexpected error, ensure temp file is removed\n",
    "        if os.path.exists(tmp):\n",
    "            try:\n",
    "                os.remove(tmp)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_if_exists(path):\n",
    "    return load_pickle(path) if os.path.exists(path) else None\n",
    "\n",
    "\n",
    "# Data loading\n",
    "def load_supertrees(dataset_names):\n",
    "    supertree_files = {\n",
    "        \"(a) Amphibians\": \"supertree_amphibians.txt\",\n",
    "        \"(b) Birds\": \"supertree_birds.txt\",\n",
    "        \"(c) Mammals\": \"supertree_mammals.txt\",\n",
    "        \"(d) Sharks\": \"supertree_sharks.txt\"\n",
    "    }\n",
    "    supertrees = []\n",
    "    for name in dataset_names:\n",
    "        with open(supertree_files[name], 'r') as f:\n",
    "            tree = Tree(f.readline().strip(), format=1)\n",
    "            supertrees.append(tree)\n",
    "    return supertrees\n",
    "\n",
    "def load_datasets(filenames):\n",
    "    datasets = []\n",
    "    for file in filenames:\n",
    "        with open(file, 'r') as f:\n",
    "            trees = [Tree(line.strip(), format=1) for line in f]\n",
    "            datasets.append(trees)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Core analysis\n",
    "\n",
    "def analyze_conflicting_pairs_per_dataset(datasets, dataset_names, supertrees,\n",
    "                                          checkpoint_every=CHECKPOINT_EVERY,\n",
    "                                          use_parallel=USE_PARALLEL,\n",
    "                                          n_jobs=N_JOBS,\n",
    "                                          report_interval=REPORT_INTERVAL):\n",
    "    # bin setup\n",
    "    bin_edges   = np.round(np.arange(0.05, 1.0, 0.1), 2)  # [0.05, 0.15, ..., 0.95]\n",
    "    bin_centers = np.round(np.arange(0.1,  1.0, 0.1), 2)  # [0.1, 0.2, ..., 0.9]\n",
    "    num_bins    = len(bin_edges) - 1\n",
    "\n",
    "    all_conflict_data = []\n",
    "    all_stats_scenario1, all_stats_scenario2, all_stats_scenario3 = [], [], []\n",
    "    conflict_counts = []\n",
    "    all_bin_pair_counts = []\n",
    "\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        dataset_name = dataset_names[idx]\n",
    "        safe_name = dataset_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\")\n",
    "        dataset_file     = f\"conflict_data_{safe_name}.pkl\"         # final per-dataset results\n",
    "        checkpoint_file = os.path.join(CHECKPOINT_DIR, f\"conflict_data_{safe_name}.checkpoint\")\n",
    "\n",
    "\n",
    "        # If finished before, just load and move on\n",
    "        if os.path.exists(dataset_file):\n",
    "            print(f\"[{dataset_name}] Loaded saved results from {dataset_file}\")\n",
    "            (conflict_data, stats1, stats2, stats3, summary, bin_pair_counts) = load_pickle(dataset_file)\n",
    "            all_conflict_data.append(conflict_data)\n",
    "            all_stats_scenario1.append(stats1)\n",
    "            all_stats_scenario2.append(stats2)\n",
    "            all_stats_scenario3.append(stats3)\n",
    "            conflict_counts.append(summary)\n",
    "            all_bin_pair_counts.append(bin_pair_counts)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{dataset_name}] Processing dataset...\")\n",
    "\n",
    "        base_tree = supertrees[idx].copy()\n",
    "\n",
    "        # Precompute leaf sets once per tree for fast Jaccard coefficient calculation\n",
    "        leaf_sets = [set(t.get_leaf_names()) for t in dataset]\n",
    "\n",
    "        # Precompute BSD(tree, base) once per tree; also record per-tree status for compatibility\n",
    "        # _bsd_from_newick returns ((bplus, bminus), status) with status in {'ok','none','exc'}\n",
    "        def _bsd_from_newick(tree_newick, base_newick):\n",
    "            try:\n",
    "                t = Tree(tree_newick, format=1)\n",
    "                base = Tree(base_newick, format=1)\n",
    "                bplus, bminus = BSD(t, base)\n",
    "                if None in (bplus, bminus):\n",
    "                    return (None, 'none')\n",
    "                return ((int(bplus), int(bminus)), 'ok')\n",
    "            except Exception:\n",
    "                return (None, 'exc')\n",
    "\n",
    "        base_newick = base_tree.write(format=1)\n",
    "        tree_newicks = [t.write(format=1) for t in dataset]\n",
    "\n",
    "        if use_parallel:\n",
    "            try:\n",
    "                from joblib import Parallel, delayed\n",
    "                print(f\"[{dataset_name}] Precomputing BSD for {len(dataset)} trees using {n_jobs} workers...\")\n",
    "                pairs = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "                    delayed(_bsd_from_newick)(nw, base_newick) for nw in tree_newicks\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[{dataset_name}] Parallel BSD precompute unavailable ({e}). Falling back to serial.\")\n",
    "                pairs = [_bsd_from_newick(nw, base_newick) for nw in tree_newicks]\n",
    "        else:\n",
    "            print(f\"[{dataset_name}] Precomputing BSD serially for {len(dataset)} trees...\")\n",
    "            pairs = [_bsd_from_newick(nw, base_newick) for nw in tree_newicks]\n",
    "\n",
    "        bsd_cache  = [p[0] for p in pairs]     # None or (bplus, bminus)\n",
    "        statuses   = [p[1] for p in pairs]     # 'ok' | 'none' | 'exc'\n",
    "\n",
    "        # Setup accumulators (resume if checkpoint exists)\n",
    "        ckpt = load_if_exists(checkpoint_file)\n",
    "        if ckpt is None:\n",
    "            bin_counts_scenario1 = [[] for _ in range(num_bins)]\n",
    "            bin_counts_scenario2 = [[] for _ in range(num_bins)]\n",
    "            bin_counts_scenario3 = [[] for _ in range(num_bins)]\n",
    "            bin_pair_counts = [0 for _ in range(num_bins)]\n",
    "\n",
    "            # Stats used for outputs\n",
    "            stats = {\n",
    "                's1': 0, 's2': 0, 's3': 0,\n",
    "                'bsd_exception_pairs': 0,\n",
    "                'bsd_none_pairs': 0,\n",
    "            }\n",
    "\n",
    "            n = len(dataset)\n",
    "            total_pairs_expected = n * (n - 1) // 2\n",
    "            total_pairs = 0      # number of valid (binned) pairs processed\n",
    "            progress_count = 0   # count of all i<j pairs visited (for progress display)\n",
    "            start_i = 0\n",
    "        else:\n",
    "            (bin_counts_scenario1, bin_counts_scenario2, bin_counts_scenario3, bin_pair_counts,\n",
    "             stats, total_pairs, progress_count, start_i) = ckpt\n",
    "            n = len(dataset)\n",
    "            total_pairs_expected = n * (n - 1) // 2\n",
    "\n",
    "        # Checkpoint-related lines\n",
    "        current_i_holder = {'i': start_i}\n",
    "        done_flag = {'done': False}\n",
    "\n",
    "        def write_ckpt_and_exit(signum=None, frame=None):\n",
    "            if not done_flag['done']:\n",
    "                save_atomic(checkpoint_file,\n",
    "                            (bin_counts_scenario1, bin_counts_scenario2, bin_counts_scenario3,\n",
    "                             bin_pair_counts, stats, total_pairs, progress_count, current_i_holder['i']))\n",
    "            if signum is not None:\n",
    "                sys.exit(1)\n",
    "\n",
    "        for sig_name in (\"SIGINT\", \"SIGTERM\"):\n",
    "            sig = getattr(signal, sig_name, None)\n",
    "            if sig is not None:\n",
    "                signal.signal(sig, write_ckpt_and_exit)\n",
    "\n",
    "        # Main pairwise loop\n",
    "        \n",
    "        for i in range(start_i, len(dataset)):\n",
    "            current_i_holder['i'] = i\n",
    "            s1 = leaf_sets[i]\n",
    "            b1 = bsd_cache[i]\n",
    "            st1 = statuses[i]\n",
    "\n",
    "            for j in range(i + 1, len(dataset)):\n",
    "                s2 = leaf_sets[j]\n",
    "\n",
    "                progress_count += 1\n",
    "                if report_interval and (progress_count % report_interval == 0):\n",
    "                    percent = (progress_count / total_pairs_expected) * 100\n",
    "                    print(f\"[{dataset_name}] {progress_count}/{total_pairs_expected} pairs visited ({percent:.1f}%)\")\n",
    "\n",
    "                # Jaccard overlap\n",
    "                inter = len(s1 & s2)\n",
    "                uni   = len(s1 | s2)\n",
    "                overlap = (inter / uni) if uni else 0.0\n",
    "\n",
    "                # Apply original overlap filter before considering BSD outcomes\n",
    "                if (overlap < 0.05) or (overlap >= 0.95):\n",
    "                    continue\n",
    "\n",
    "                # Determine bin\n",
    "                bin_index = np.digitize([overlap], bin_edges)[0] - 1\n",
    "                if bin_index < 0 or bin_index >= num_bins:\n",
    "                    continue  # safety\n",
    "\n",
    "                st2 = statuses[j]\n",
    "                b2  = bsd_cache[j]\n",
    "\n",
    "                # Reproduce original per-pair failure bookkeeping after overlap filter\n",
    "                if st1 == 'exc' or st2 == 'exc':\n",
    "                    stats['bsd_exception_pairs'] += 1\n",
    "                    continue\n",
    "                if st1 == 'none' or st2 == 'none':\n",
    "                    stats['bsd_none_pairs'] += 1\n",
    "                    continue\n",
    "\n",
    "                # If Both BSD available, then proceed to scenarios\n",
    "                d1_T1, d2_T1 = b1[0], b1[1]\n",
    "                d1_T2, d2_T2 = b2[0], b2[1]\n",
    "\n",
    "                total_pairs += 1\n",
    "                bin_pair_counts[bin_index] += 1\n",
    "\n",
    "                # Scenario 1\n",
    "                if (d2_T1 < d2_T2 and d1_T1 > d1_T2) or (d2_T2 < d2_T1 and d1_T2 > d1_T1):\n",
    "                    stats['s1'] += 1\n",
    "                    bin_counts_scenario1[bin_index].append(1)\n",
    "                else:\n",
    "                    bin_counts_scenario1[bin_index].append(0)\n",
    "\n",
    "                # Scenario 2\n",
    "                if d2_T1 == d2_T2 and d1_T1 != d1_T2:\n",
    "                    stats['s2'] += 1\n",
    "                    bin_counts_scenario2[bin_index].append(1)\n",
    "                else:\n",
    "                    bin_counts_scenario2[bin_index].append(0)\n",
    "\n",
    "                # Scenario 3\n",
    "                if d2_T1 != d2_T2 and d1_T1 == d1_T2:\n",
    "                    stats['s3'] += 1\n",
    "                    bin_counts_scenario3[bin_index].append(1)\n",
    "                else:\n",
    "                    bin_counts_scenario3[bin_index].append(0)\n",
    "\n",
    "            # Save a checkpoint after each i (or every N trees)\n",
    "            if (i + 1) % checkpoint_every == 0:\n",
    "                save_atomic(checkpoint_file,\n",
    "                            (bin_counts_scenario1, bin_counts_scenario2, bin_counts_scenario3,\n",
    "                             bin_pair_counts, stats, total_pairs, progress_count, i + 1))\n",
    "                print(f\"[{dataset_name}] Checkpoint saved at i={i + 1}/{len(dataset)}\")\n",
    "\n",
    "        # Finished dataset\n",
    "        print(f\"[{dataset_name}] Finished processing {total_pairs} valid tree pairs.\")\n",
    "        done_flag['done'] = True  # suppress further checkpoint writes in handlers\n",
    "\n",
    "        # Summaries\n",
    "        \n",
    "        def compute_stats(bin_data):\n",
    "            out = {'mean': [], 'std': [], 'median': [], 'q1': [], 'q3': [], 'ci_lower': [], 'ci_upper': []}\n",
    "            for data in bin_data:\n",
    "                if data:\n",
    "                    arr = np.array(data)\n",
    "                    out['mean'].append(np.mean(arr))\n",
    "                    out['std'].append(np.std(arr, ddof=1) if len(arr) > 1 else 0)\n",
    "                    out['median'].append(np.median(arr))\n",
    "                    out['q1'].append(np.percentile(arr, 25))\n",
    "                    out['q3'].append(np.percentile(arr, 75))\n",
    "                    ci = np.percentile(arr, [2.5, 97.5])\n",
    "                    out['ci_lower'].append(ci[0])\n",
    "                    out['ci_upper'].append(ci[1])\n",
    "                else:\n",
    "                    for k in out:\n",
    "                        out[k].append(0)\n",
    "            return out\n",
    "\n",
    "        stats1 = compute_stats(bin_counts_scenario1)\n",
    "        stats2 = compute_stats(bin_counts_scenario2)\n",
    "        stats3 = compute_stats(bin_counts_scenario3)\n",
    "\n",
    "        conflict_data = {\n",
    "            'fractions_scenario1': [np.mean(b) if b else 0 for b in bin_counts_scenario1],\n",
    "            'fractions_scenario2': [np.mean(b) if b else 0 for b in bin_counts_scenario2],\n",
    "            'fractions_scenario3': [np.mean(b) if b else 0 for b in bin_counts_scenario3]\n",
    "        }\n",
    "\n",
    "        # Build summary\n",
    "        summary = {\n",
    "            'Scenario 1': stats['s1'],\n",
    "            'Scenario 2': stats['s2'],\n",
    "            'Scenario 3': stats['s3'],\n",
    "            'Total Pairs Processed': total_pairs,\n",
    "            'Skipped Pairs (Zero Overlap)': 0,\n",
    "            'Skipped Pairs (Full Overlap)': 0,\n",
    "            'Skipped Pairs (BSD None)': stats['bsd_none_pairs'],\n",
    "            'Skipped Pairs (BSD Exceptions)': stats['bsd_exception_pairs'],\n",
    "        }\n",
    "\n",
    "        # Save final per-dataset result and remove checkpoint\n",
    "        save_atomic(dataset_file, (conflict_data, stats1, stats2, stats3, summary, bin_pair_counts))\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            try:\n",
    "                os.remove(checkpoint_file)\n",
    "            except OSError:\n",
    "                pass\n",
    "        print(f\"[{dataset_name}] Results saved to {dataset_file}\\n\")\n",
    "\n",
    "        # Collect for return\n",
    "        all_conflict_data.append(conflict_data)\n",
    "        all_stats_scenario1.append(stats1)\n",
    "        all_stats_scenario2.append(stats2)\n",
    "        all_stats_scenario3.append(stats3)\n",
    "        conflict_counts.append(summary)\n",
    "        all_bin_pair_counts.append(bin_pair_counts)\n",
    "\n",
    "    return bin_centers, conflict_counts, all_conflict_data, all_stats_scenario1, all_stats_scenario2, all_stats_scenario3, all_bin_pair_counts\n",
    "\n",
    "\n",
    "# Reporting \n",
    "def create_conflict_dataframe(conflict_counts, dataset_names):\n",
    "    df = pd.DataFrame(conflict_counts, index=dataset_names)\n",
    "    print(\"\\nConflict Summary:\")\n",
    "    print(df.T)\n",
    "    return df\n",
    "\n",
    "def plot_conflicts_by_scenario(overlap_ranges, all_conflict_data, all_stats_scenario1, all_stats_scenario2, all_stats_scenario3, dataset_names, all_bin_pair_counts):\n",
    "    scenarios = [\n",
    "        ('Scenario 1', 'fractions_scenario1', all_stats_scenario1),\n",
    "        ('Scenario 2', 'fractions_scenario2', all_stats_scenario2),\n",
    "        ('Scenario 3', 'fractions_scenario3', all_stats_scenario3),\n",
    "    ]\n",
    "    group_colors = ['tab:blue', 'tab:green', 'tab:orange', 'tab:red']\n",
    "\n",
    "    for sidx, (sname, frac_key, stat_list) in enumerate(scenarios):\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        for idx, group in enumerate(dataset_names):\n",
    "            y = all_conflict_data[idx][frac_key]\n",
    "            plt.plot(overlap_ranges, y, label=group, marker='o', color=group_colors[idx])\n",
    "            for x, val, cnt in zip(overlap_ranges, y, all_bin_pair_counts[idx]):\n",
    "                plt.text(x, val, f\"{cnt}\", color=group_colors[idx], fontsize=9, ha='center', va='bottom')\n",
    "        plt.xlabel('Overlap ratio', fontsize=14)\n",
    "        plt.ylabel('Fraction of conflicts', fontsize=14)\n",
    "        plt.title(f\"{sname}: Conflict Fractions by Group\", fontsize=16)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "        plt.grid(True)\n",
    "        plt.legend(title='Group', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{sname.replace(\" \", \"_\").lower()}_conflicts_by_group.svg')\n",
    "        plt.savefig(f'{sname.replace(\" \", \"_\").lower()}_conflicts_by_group.pdf')\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for idx, group in enumerate(dataset_names):\n",
    "        y_total = (np.array(all_conflict_data[idx]['fractions_scenario1']) +\n",
    "                   np.array(all_conflict_data[idx]['fractions_scenario2']) +\n",
    "                   np.array(all_conflict_data[idx]['fractions_scenario3']))\n",
    "        plt.plot(overlap_ranges, y_total, label=group, marker='o', color=group_colors[idx])\n",
    "        for x, val, cnt in zip(overlap_ranges, y_total, all_bin_pair_counts[idx]):\n",
    "            plt.text(x, val, f\"{cnt}\", color=group_colors[idx], fontsize=9, ha='center', va='bottom')\n",
    "    plt.xlabel('Overlap ratio', fontsize=14)\n",
    "    plt.ylabel('Total fraction of conflicts', fontsize=14)\n",
    "    plt.title('All Scenarios: Total Conflict Fractions by Group', fontsize=16)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='Group', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_scenarios_conflicts_by_group2.svg')\n",
    "    plt.savefig('all_scenarios_conflicts_by_group2.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    data_file = 'conflict_data_per_dataset.pkl'\n",
    "    dataset_names = [\"(a) Amphibians\", \"(b) Birds\", \"(c) Mammals\", \"(d) Sharks\"]\n",
    "    raw_dataset_names = [\"amphibians170.txt\", \"birds100.txt\", \"mammals140.txt\", \"sharks100.txt\"]\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        with open(data_file, 'rb') as f:\n",
    "            (overlap_bins, conflict_counts, all_conflict_data,\n",
    "             all_stats_scenario1, all_stats_scenario2, all_stats_scenario3, all_bin_pair_counts) = pickle.load(f)\n",
    "        print(f\"Loaded aggregated results from {data_file}\")\n",
    "    else:\n",
    "        datasets = load_datasets(raw_dataset_names)\n",
    "        supertrees = load_supertrees(dataset_names)\n",
    "        (overlap_bins, conflict_counts, all_conflict_data,\n",
    "         all_stats_scenario1, all_stats_scenario2, all_stats_scenario3, all_bin_pair_counts) = \\\n",
    "            analyze_conflicting_pairs_per_dataset(datasets, dataset_names, supertrees)\n",
    "        save_atomic(data_file, (overlap_bins, conflict_counts, all_conflict_data,\n",
    "                                all_stats_scenario1, all_stats_scenario2, all_stats_scenario3, all_bin_pair_counts))\n",
    "        print(f\"Aggregated results saved to {data_file}\")\n",
    "\n",
    "    create_conflict_dataframe(conflict_counts, dataset_names)\n",
    "    plot_conflicts_by_scenario(\n",
    "        overlap_bins, all_conflict_data,\n",
    "        all_stats_scenario1, all_stats_scenario2, all_stats_scenario3,\n",
    "        dataset_names, all_bin_pair_counts\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Inputs expected in the current working directory:\n",
    "  - Preferred: conflict_data_per_dataset.pkl\n",
    "    Tuple: (overlap_bins, conflict_counts, all_conflict_data,\n",
    "            all_stats_scenario1, all_stats_scenario2, all_stats_scenario3,\n",
    "            all_bin_pair_counts)\n",
    "  - Fallback (if the file above is absent) — four per-dataset pickles:\n",
    "    - conflict_data_a_Amphibians.pkl\n",
    "    - conflict_data_b_Birds.pkl\n",
    "    - conflict_data_c_Mammals.pkl\n",
    "    - conflict_data_d_Sharks.pkl\n",
    "    Each is a 6-tuple: (conflict_data, stats1, stats2, stats3, summary, bin_pair_counts)\n",
    "    where conflict_data has keys:\n",
    "      'fractions_scenario1', 'fractions_scenario2', 'fractions_scenario3'\n",
    "\n",
    "Outputs:\n",
    "  - all_scenarios_line_plus_violins.svg\n",
    "  - all_scenarios_line_plus_violins.pdf\n",
    "  - and individual panels:\n",
    "      conflicts_line_top.(svg/pdf)\n",
    "      conflicts_violins_bottom.(svg/pdf)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _install_numpy_core_aliases():\n",
    "    \"\"\"Make NumPy 1.x compatible with pickles saved under NumPy 2.x (numpy._core...).\"\"\"\n",
    "    try:\n",
    "        import numpy as _np\n",
    "        if 'numpy._core' not in sys.modules:\n",
    "            sys.modules['numpy._core'] = _np.core\n",
    "        for sub in ['multiarray', 'numeric', 'overrides', 'umath']:\n",
    "            key = f'numpy._core.{sub}'\n",
    "            if key not in sys.modules and hasattr(_np.core, sub):\n",
    "                sys.modules[key] = getattr(_np.core, sub)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def compat_pickle_load(path):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except ModuleNotFoundError as e:\n",
    "        if 'numpy._core' in str(e):\n",
    "            _install_numpy_core_aliases()\n",
    "            with open(path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        raise\n",
    "\n",
    "# Data loading\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      overlap_bins            : np.ndarray of bin centers (e.g., 0.1..0.9)\n",
    "      dataset_names           : list of strings in the original label order\n",
    "      all_conflict_data       : list of dicts (one per dataset) with scenario fraction arrays\n",
    "      all_bin_pair_counts     : list of lists, per dataset -> per bin counts\n",
    "    \"\"\"\n",
    "    agg = \"conflict_data_per_dataset.pkl\"\n",
    "    dataset_names = [\"(a) Amphibians\", \"(b) Birds\", \"(c) Mammals\", \"(d) Sharks\"]\n",
    "\n",
    "    if os.path.exists(agg):\n",
    "        (overlap_bins, _conflict_counts, all_conflict_data,\n",
    "         _s1, _s2, _s3, all_bin_pair_counts) = compat_pickle_load(agg)\n",
    "        return np.asarray(overlap_bins), dataset_names, all_conflict_data, all_bin_pair_counts\n",
    "\n",
    "    # Fallback to per-dataset files\n",
    "    parts = [\n",
    "        \"conflict_data_a_Amphibians.pkl\",\n",
    "        \"conflict_data_b_Birds.pkl\",\n",
    "        \"conflict_data_c_Mammals.pkl\",\n",
    "        \"conflict_data_d_Sharks.pkl\",\n",
    "    ]\n",
    "    all_conflict_data = []\n",
    "    all_bin_pair_counts = []\n",
    "    for p in parts:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing '{agg}' and per-dataset file '{p}'. \"\n",
    "                \"Place the pickles in this directory or update the paths.\"\n",
    "            )\n",
    "        conflict_data, stats1, stats2, stats3, summary, bin_pair_counts = compat_pickle_load(p)\n",
    "        all_conflict_data.append(conflict_data)\n",
    "        all_bin_pair_counts.append(bin_pair_counts)\n",
    "\n",
    "    # Infer bin centers if using fallback\n",
    "    overlap_bins = np.round(np.arange(0.1, 1.0, 0.1), 2)\n",
    "    return overlap_bins, dataset_names, all_conflict_data, all_bin_pair_counts\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def clean_group_name(name: str) -> str:\n",
    "    \"\"\"Remove the leading '(a) ' etc from dataset names for legends.\"\"\"\n",
    "    return re.sub(r\"^\\([a-d]\\)\\s*\", \"\", name, flags=re.IGNORECASE)\n",
    "\n",
    "def compute_totals(all_conflict_data):\n",
    "    \"\"\"Sum fractions across the three scenarios for each dataset.\"\"\"\n",
    "    totals = []\n",
    "    for d in all_conflict_data:\n",
    "        y_total = (np.array(d['fractions_scenario1']) +\n",
    "                   np.array(d['fractions_scenario2']) +\n",
    "                   np.array(d['fractions_scenario3']))\n",
    "        totals.append(np.asarray(y_total))\n",
    "    return totals  # list length=4, each an array length n_bins\n",
    "\n",
    "# Plotting\n",
    "\n",
    "def plot_top_line(ax, overlap_bins, dataset_names, totals_by_group, all_bin_pair_counts, panel_label=\"(a)\"):\n",
    "    \"\"\"\n",
    "    Draw line chart and overlay per-bin counts (colored to match the line).\n",
    "    Legend is under the plot.\n",
    "    \"\"\"\n",
    "    # Plot lines and annotate counts\n",
    "    for name, y, bin_counts in zip(dataset_names, totals_by_group, all_bin_pair_counts):\n",
    "        (ln,) = ax.plot(overlap_bins, y, marker='o', label=clean_group_name(name))\n",
    "        color = ln.get_color()\n",
    "        # numbers at each point\n",
    "        for x, val, cnt in zip(overlap_bins, y, bin_counts):\n",
    "            ax.text(x, val, f\"{cnt}\", color=color, fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "    ax.set_xlabel(r\"Overlap level $p$\")\n",
    "    ax.set_ylabel(\"Total fraction of conflicts\")\n",
    "    ax.set_title(\"(a)\", loc=\"left\", pad=6)\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 0.2)\n",
    "    ax.set_xticks(np.round(np.arange(0.1, 1.0, 0.1), 2))\n",
    "    ax.set_yticks([0.0, 0.1, 0.2])\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "\n",
    "    # Legend under the plot\n",
    "    ax.legend(\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        ncol=len(dataset_names),\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "def plot_bottom_violins(ax, overlap_bins, totals_by_group, panel_label=\"(b)\",\n",
    "                        jitter=True, show_means=False, show_medians=True,\n",
    "                        median_color=\"black\", median_linewidth=1.6):\n",
    "    \"\"\"\n",
    "    For each overlap bin k, build a small distribution from the 4 groups' totals -> violin at x=bin center.\n",
    "    Shows only the median line by default.\n",
    "    \"\"\"\n",
    "    data_per_bin = [[tg[k] for tg in totals_by_group] for k in range(len(overlap_bins))]\n",
    "\n",
    "    parts = ax.violinplot(\n",
    "        dataset=data_per_bin,\n",
    "        positions=overlap_bins,\n",
    "        showmeans=show_means,\n",
    "        showmedians=show_medians,\n",
    "        showextrema=False,\n",
    "        widths=0.07\n",
    "    )\n",
    "\n",
    "    # Style the median line for clarity\n",
    "    if show_medians and isinstance(parts, dict) and 'cmedians' in parts and parts['cmedians'] is not None:\n",
    "        parts['cmedians'].set_color(median_color)\n",
    "        parts['cmedians'].set_linewidth(median_linewidth)\n",
    "\n",
    "    # Overlay the actual group values per bin\n",
    "    for x, ys in zip(overlap_bins, data_per_bin):\n",
    "        for y in ys:\n",
    "            ax.scatter(x, y, s=18)\n",
    "\n",
    "    ax.set_xlabel(r\"Overlap level $p$\")\n",
    "    ax.set_ylabel(\"Total fraction of conflicts\")\n",
    "    ax.set_title(\"(b)\", loc=\"left\", pad=6)\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 0.2)\n",
    "    ax.set_xticks(np.round(np.arange(0.1, 1.0, 0.1), 2))\n",
    "    ax.set_yticks([0.0, 0.1, 0.2])\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "\n",
    "# Main\n",
    "\n",
    "def main():\n",
    "    overlap_bins, dataset_names, all_conflict_data, all_bin_pair_counts = load_data()\n",
    "    totals_by_group = compute_totals(all_conflict_data)\n",
    "\n",
    "    # Create a single figure with two vertical panels\n",
    "    fig = plt.figure(figsize=(10, 9))\n",
    "    gs = fig.add_gridspec(nrows=2, ncols=1, height_ratios=[1, 1.1])\n",
    "\n",
    "    ax_top = fig.add_subplot(gs[0, 0])\n",
    "    plot_top_line(ax_top, overlap_bins, dataset_names, totals_by_group, all_bin_pair_counts, panel_label=\"(a)\")\n",
    "\n",
    "    ax_bottom = fig.add_subplot(gs[1, 0])\n",
    "    # Median only\n",
    "    plot_bottom_violins(\n",
    "        ax_bottom, overlap_bins, totals_by_group,\n",
    "        panel_label=\"(b)\", jitter=True,\n",
    "        show_means=False, show_medians=True,\n",
    "        median_color=\"black\", median_linewidth=1.6\n",
    "    )\n",
    "\n",
    "    # Extra bottom margin\n",
    "    fig.subplots_adjust(hspace=0.35, bottom=0.14, top=0.95)\n",
    "\n",
    "    # Save combined outputs\n",
    "    out_svg = \"all_scenarios_line_plus_violins_median.svg\"\n",
    "    out_pdf = \"all_scenarios_line_plus_violins_median.pdf\"\n",
    "    fig.savefig(out_svg, bbox_inches=\"tight\")\n",
    "    fig.savefig(out_pdf, bbox_inches=\"tight\")\n",
    "\n",
    "    # (Optional) also save each panel separately\n",
    "    # Top only\n",
    "    fig_top, ax_t = plt.subplots(figsize=(10, 4.8))\n",
    "    plot_top_line(ax_t, overlap_bins, dataset_names, totals_by_group, all_bin_pair_counts, panel_label=\"(a)\")\n",
    "    fig_top.tight_layout()\n",
    "    fig_top.savefig(\"conflicts_line_top_median.svg\", bbox_inches=\"tight\")\n",
    "    fig_top.savefig(\"conflicts_line_top_median.pdf\", bbox_inches=\"tight\")\n",
    "    #fig_top.savefig(\"conflicts_line_top.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig_top)\n",
    "\n",
    "    # Bottom only\n",
    "    fig_bot, ax_b = plt.subplots(figsize=(10, 5.2))\n",
    "    plot_bottom_violins(\n",
    "        ax_b, overlap_bins, totals_by_group,\n",
    "        panel_label=\"(b)\", jitter=True,\n",
    "        show_means=False, show_medians=True,\n",
    "        median_color=\"black\", median_linewidth=1.6\n",
    "    )\n",
    "    fig_bot.tight_layout()\n",
    "    fig_bot.savefig(\"conflicts_violins_bottom_median.svg\", bbox_inches=\"tight\")\n",
    "    fig_bot.savefig(\"conflicts_violins_bottom_median.pdf\", bbox_inches=\"tight\")\n",
    "    #fig_bot.savefig(\"conflicts_violins_bottom.png\", bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close(fig_bot)\n",
    "\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved:\\n  {out_svg}\\n  {out_pdf}\\n\"\n",
    "          f\"  conflicts_line_top.(svg/pdf)\\n  conflicts_violins_bottom.(svg/pdf)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUTWLWnzllyWywtgBQEwUv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ete-env)",
   "language": "python",
   "name": "ete-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
